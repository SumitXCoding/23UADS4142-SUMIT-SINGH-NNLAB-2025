{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4caeb5d",
   "metadata": {},
   "source": [
    "# Evaluation of a Three-Layer Neural Network in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbcc62",
   "metadata": {},
   "source": [
    "## Objective\n",
    "To evaluate the performance of a three-layer neural network with variations in activation functions, size of hidden layers, learning rate, batch size, and number of epochs using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e1353",
   "metadata": {},
   "source": [
    "## Description of the Model\n",
    "This neural network consists of:\n",
    "- Input layer with 784 neurons (28x28 images)\n",
    "- Two hidden layers (sizes vary in experiments)\n",
    "- Output layer with 10 neurons (one per class)\n",
    "\n",
    "Experiments vary activation functions (`ReLU`, `Sigmoid`), hidden layer sizes, learning rate, batch size, and number of epochs to evaluate performance impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7fb81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "x_test = x_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "y_test = tf.one_hot(y_test, depth=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc294c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(hidden1=128, hidden2=64, activation=tf.nn.relu, lr=0.01, epochs=10, batch_size=100):\n",
    "    input_size = 784\n",
    "    output_size = 10\n",
    "\n",
    "    W1 = tf.Variable(tf.random.normal([input_size, hidden1], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.zeros([hidden1]))\n",
    "    W2 = tf.Variable(tf.random.normal([hidden1, hidden2], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.zeros([hidden2]))\n",
    "    W3 = tf.Variable(tf.random.normal([hidden2, output_size], stddev=0.1))\n",
    "    b3 = tf.Variable(tf.zeros([output_size]))\n",
    "\n",
    "    def forward(x):\n",
    "        z1 = tf.matmul(x, W1) + b1\n",
    "        a1 = activation(z1)\n",
    "        z2 = tf.matmul(a1, W2) + b2\n",
    "        a2 = activation(z2)\n",
    "        return tf.matmul(a2, W3) + b3\n",
    "\n",
    "    def compute_loss(logits, labels):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "    def compute_accuracy(logits, labels):\n",
    "        preds = tf.argmax(logits, axis=1)\n",
    "        actual = tf.argmax(labels, axis=1)\n",
    "        return tf.reduce_mean(tf.cast(tf.equal(preds, actual), tf.float32))\n",
    "\n",
    "    num_batches = x_train.shape[0] // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            start, end = i * batch_size, (i + 1) * batch_size\n",
    "            x_batch = x_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = forward(x_batch)\n",
    "                loss = compute_loss(logits, y_batch)\n",
    "            grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "            for var, grad in zip([W1, b1, W2, b2, W3, b3], grads):\n",
    "                var.assign_sub(lr * grad)\n",
    "            avg_loss += loss.numpy()\n",
    "\n",
    "        test_logits = forward(x_test)\n",
    "        test_acc = compute_accuracy(test_logits, y_test)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {test_acc.numpy():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e176389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 829.9273, Accuracy: 0.8280\n",
      "Epoch 2/10, Loss: 322.9185, Accuracy: 0.8793\n",
      "Epoch 3/10, Loss: 247.2450, Accuracy: 0.8952\n",
      "Epoch 4/10, Loss: 216.1263, Accuracy: 0.9055\n",
      "Epoch 5/10, Loss: 196.8300, Accuracy: 0.9113\n",
      "Epoch 6/10, Loss: 182.5364, Accuracy: 0.9183\n",
      "Epoch 7/10, Loss: 171.0022, Accuracy: 0.9229\n",
      "Epoch 8/10, Loss: 161.2271, Accuracy: 0.9257\n",
      "Epoch 9/10, Loss: 152.7163, Accuracy: 0.9292\n",
      "Epoch 10/10, Loss: 145.1577, Accuracy: 0.9316\n",
      "Epoch 1/10, Loss: 923.6167, Accuracy: 0.8587\n",
      "Epoch 2/10, Loss: 417.6165, Accuracy: 0.8958\n",
      "Epoch 3/10, Loss: 339.0749, Accuracy: 0.9097\n",
      "Epoch 4/10, Loss: 300.9982, Accuracy: 0.9166\n",
      "Epoch 5/10, Loss: 275.8647, Accuracy: 0.9227\n",
      "Epoch 6/10, Loss: 256.8271, Accuracy: 0.9271\n",
      "Epoch 7/10, Loss: 241.4531, Accuracy: 0.9305\n",
      "Epoch 8/10, Loss: 228.4697, Accuracy: 0.9340\n",
      "Epoch 9/10, Loss: 217.1358, Accuracy: 0.9367\n",
      "Epoch 10/10, Loss: 207.0874, Accuracy: 0.9384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try different activation functions and hidden layer sizes\n",
    "train_model(hidden1=128, hidden2=64, activation=tf.nn.relu, lr=0.01, epochs=10, batch_size=100)\n",
    "train_model(hidden1=256, hidden2=128, activation=tf.nn.relu, lr=0.005, epochs=10, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead4eff",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "Each experiment logs the test accuracy and loss per epoch. You can visualize or extend this to include confusion matrix or loss curve plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b8384",
   "metadata": {},
   "source": [
    "## My Comments\n",
    "- Accuracy improves with more neurons and proper activation.\n",
    "- Sigmoid gives lower performance compared to ReLU due to vanishing gradients.\n",
    "- Learning rate and batch size significantly affect convergence speed.\n",
    "- Can be further improved by adding dropout or batch normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
