{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91d8b4c",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron (MLP) Description\n",
    "\n",
    "A **Multi-Layer Perceptron (MLP)** is a type of artificial neural network that consists of multiple layers of neurons. Unlike a single-layer perceptron, which can only solve linearly separable problems, an MLP can learn complex patterns, including the XOR Boolean function.\n",
    "\n",
    "#### Structure of MLP\n",
    "1. **Input Layer**  \n",
    "   - Receives input features (e.g., two binary inputs for XOR: [0,0], [0,1], etc.).\n",
    "   - Each neuron in this layer passes values to the next layer.\n",
    "\n",
    "2. **Hidden Layer**  \n",
    "   - This layer processes inputs using weights and biases.\n",
    "   - Each neuron in the hidden layer applies an **activation function** to determine its output.\n",
    "   - For this experiment, we use the **step function** instead of the traditional sigmoid.\n",
    "\n",
    "3. **Output Layer**  \n",
    "   - Produces the final predicted output.\n",
    "   - Uses the step function to return either 0 or 1.\n",
    "\n",
    "#### How MLP Learns\n",
    "1. **Forward Propagation:**  \n",
    "   - Inputs are passed through weighted connections, summed with biases, and processed through activation functions.  \n",
    "   - The network calculates the final output.\n",
    "\n",
    "2. **Error Calculation:**  \n",
    "   - The difference between the predicted and actual output is determined.  \n",
    "\n",
    "3. **Weight Updates (Backpropagation-like Process):**  \n",
    "   - Since we are not using a sigmoid function, a simplified update method is applied:\n",
    "     - The output error is propagated backward.  \n",
    "     - Weights and biases are adjusted using the **learning rate** to minimize the error.  \n",
    "\n",
    "#### Why MLP Works for XOR\n",
    "- The XOR function is **not linearly separable**, meaning a single-layer perceptron cannot classify it correctly.  \n",
    "- The **hidden layer** enables MLP to model more complex decision boundaries.  \n",
    "- By learning an intermediate representation, the network can correctly map XOR inputs to outputs.\n",
    "\n",
    "#### Performance Evaluation\n",
    "- We measure accuracy using `accuracy_score()`.  \n",
    "- A **confusion matrix** visualizes the classification performance.  \n",
    "- If the accuracy is 1.0, it means the MLP has learned XOR perfectly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a94c6b",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP) Learning Algorithm\n",
    "## Objectives\n",
    "- Implement the Multi-Layer Perceptron (MLP) Learning Algorithm using NumPy in Python.\n",
    "- Evaluate the performance of a single perceptron for **NAND** and **XOR** truth tables.\n",
    "- Use the **Step Function** as the activation function.\n",
    "- Visualize the **Confusion Matrix**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa7ae1d",
   "metadata": {},
   "source": [
    "## Description of the Model\n",
    "The perceptron is a simple linear classifier that uses weights and biases to make predictions based on input data. It follows these steps:\n",
    "1. Compute weighted sum of inputs.\n",
    "2. Apply a step activation function.\n",
    "3. Update weights using learning rate and error correction.\n",
    "4. Repeat for multiple epochs.\n",
    "\n",
    "**Note:** A single-layer perceptron **cannot** solve the XOR problem due to non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b741e",
   "metadata": {},
   "source": [
    "## NAND Perceptron Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe70d1d",
   "metadata": {},
   "source": [
    "## XOR Perceptron Implementation (Fails due to Linearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f303d",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "- **Confusion Matrix** is used to visualize classification performance.\n",
    "- **Accuracy** metric shows correct predictions.\n",
    "\n",
    "**Observations:**\n",
    "- The perceptron successfully classifies NAND with 100% accuracy.\n",
    "- The perceptron fails for XOR due to its inability to handle non-linearly separable data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22485a1a",
   "metadata": {},
   "source": [
    "## My Comments (Limitations & Improvements)\n",
    "- The single-layer perceptron **cannot** classify XOR correctly.\n",
    "- To solve XOR, a **multi-layer perceptron (MLP) with a hidden layer** is required.\n",
    "- **Activation functions like ReLU or Sigmoid** can improve learning in non-linear problems.\n",
    "- A perceptron is best suited for **linearly separable data**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Step Activation Function\n",
    "def step_function(x):\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "# MLP Training Function (without sigmoid)\n",
    "def train_mlp(X, y, hidden_neurons=2, epochs=10000, lr=0.1):\n",
    "    input_neurons = X.shape[1]\n",
    "    output_neurons = 1\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    w_hidden = np.random.uniform(-1, 1, size=(input_neurons, hidden_neurons))\n",
    "    b_hidden = np.zeros((1, hidden_neurons))\n",
    "    w_output = np.random.uniform(-1, 1, size=(hidden_neurons, output_neurons))\n",
    "    b_output = np.zeros((1, output_neurons))\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        # Forward pass\n",
    "        hidden_input = np.dot(X, w_hidden) + b_hidden\n",
    "        hidden_output = step_function(hidden_input)\n",
    "        final_input = np.dot(hidden_output, w_output) + b_output\n",
    "        final_output = step_function(final_input)\n",
    "\n",
    "        # Compute error\n",
    "        error = y - final_output\n",
    "\n",
    "        # Backpropagation-like weight updates\n",
    "        d_output = error\n",
    "        d_hidden = d_output.dot(w_output.T)\n",
    "\n",
    "        # Update weights and biases using learning rate\n",
    "        w_output += hidden_output.T.dot(d_output) * lr\n",
    "        b_output += np.sum(d_output, axis=0, keepdims=True) * lr\n",
    "        w_hidden += X.T.dot(d_hidden) * lr\n",
    "        b_hidden += np.sum(d_hidden, axis=0, keepdims=True) * lr\n",
    "\n",
    "    return w_hidden, b_hidden, w_output, b_output\n",
    "\n",
    "# MLP Prediction Function\n",
    "def predict_mlp(X, w_hidden, b_hidden, w_output, b_output):\n",
    "    hidden_input = np.dot(X, w_hidden) + b_hidden\n",
    "    hidden_output = step_function(hidden_input)\n",
    "    final_input = np.dot(hidden_output, w_output) + b_output\n",
    "    return step_function(final_input)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# XOR Dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Train MLP\n",
    "w_hidden, b_hidden, w_output, b_output = train_mlp(X_xor, y_xor)\n",
    "\n",
    "# Predict on XOR dataset\n",
    "y_pred_xor = predict_mlp(X_xor, w_hidden, b_hidden, w_output, b_output)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_xor = accuracy_score(y_xor, y_pred_xor)\n",
    "print(\"XOR MLP Accuracy:\", accuracy_xor)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plot_confusion_matrix(y_xor, y_pred_xor, title=\"XOR MLP Confusion Matrix\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
